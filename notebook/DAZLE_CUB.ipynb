{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-1])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)\n",
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.CUBDataLoader import CUBDataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention,eval_zs_gzsl#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path\n",
    "import importlib\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_GPU = 4\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "CUB\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/CUB/feature_map_ResNet_101_CUB.hdf5\n",
      "Expert Attr\n",
      "Finish loading data in  61.433818\n"
     ]
    }
   ],
   "source": [
    "dataloader = CUBDataLoader(NFS_path,device,is_unsupervised_attr=False,is_balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.augment_img_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = nn.Parameter(F.normalize(torch.tensor(att)),requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Add one smoothing\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "{'pmp': {'init_lambda': 0.1, 'final_lambda': 0.1, 'phase': 0.8}, 'desired_mass': {'init_lambda': -1, 'final_lambda': -1, 'phase': 0.8}}\n",
      "\t V\n",
      "\t W_1\n",
      "\t W_2\n",
      "\t W_3\n",
      "default lr ['V'] 1x lr ['W_1', 'W_2', 'W_3']\n",
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_ 0.1\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.9 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 214#215#\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att\n",
    "normalize_att = dataloader.normalize_att\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1#0.1\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1\n",
    "report_interval = niters//nepoches\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=False,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True)\n",
    "model.to(device)\n",
    "\n",
    "setup = {'pmp':{'init_lambda':0.1,'final_lambda':0.1,'phase':0.8},\n",
    "         'desired_mass':{'init_lambda':-1,'final_lambda':-1,'phase':0.8}}\n",
    "print(setup)\n",
    "#scheduler = Scheduler(model,niters,batch_size,report_interval,setup)\n",
    "\n",
    "params_to_update = []\n",
    "params_names = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        params_names.append(name)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.9#0.#\n",
    "#%%\n",
    "lr_seperator = 1\n",
    "lr_factor = 1\n",
    "print('default lr {} {}x lr {}'.format(params_names[:lr_seperator],lr_factor,params_names[lr_seperator:]))\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss': 5.339369297027588, 'loss_CE': 5.305324554443359, 'loss_cal': 0.34044915437698364, 'acc_seen': 0, 'acc_novel': 0, 'H': 0, 'acc_zs': 0}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 141, 'loss': 1.6474179029464722, 'loss_CE': 1.5508091449737549, 'loss_cal': 0.9660871028900146, 'acc_seen': 0.32329174876213074, 'acc_novel': 0.4783506989479065, 'H': 0.3858249633018277, 'acc_zs': 0.5206593871116638}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 282, 'loss': 1.5724619626998901, 'loss_CE': 1.4667860269546509, 'loss_cal': 1.0567588806152344, 'acc_seen': 0.44570451974868774, 'acc_novel': 0.5117195248603821, 'H': 0.4764361337239027, 'acc_zs': 0.579412579536438}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 423, 'loss': 1.3222812414169312, 'loss_CE': 1.2042889595031738, 'loss_cal': 1.179922342300415, 'acc_seen': 0.5000437498092651, 'acc_novel': 0.537502646446228, 'H': 0.5180970023728759, 'acc_zs': 0.610663890838623}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 564, 'loss': 1.115435004234314, 'loss_CE': 1.0016543865203857, 'loss_cal': 1.137805700302124, 'acc_seen': 0.5307056903839111, 'acc_novel': 0.5414375066757202, 'H': 0.5360178874764615, 'acc_zs': 0.6320300102233887}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 705, 'loss': 1.1483986377716064, 'loss_CE': 1.0214109420776367, 'loss_cal': 1.2698768377304077, 'acc_seen': 0.5512126684188843, 'acc_novel': 0.5535522103309631, 'H': 0.5523799621707404, 'acc_zs': 0.645402193069458}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 846, 'loss': 1.0018386840820312, 'loss_CE': 0.853941798210144, 'loss_cal': 1.4789692163467407, 'acc_seen': 0.5494822263717651, 'acc_novel': 0.5576741695404053, 'H': 0.5535478915182929, 'acc_zs': 0.6444504261016846}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 987, 'loss': 0.9130017161369324, 'loss_CE': 0.7828669548034668, 'loss_cal': 1.3013474941253662, 'acc_seen': 0.5765684247016907, 'acc_novel': 0.5499624013900757, 'H': 0.5629512270244877, 'acc_zs': 0.6390618681907654}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1128, 'loss': 1.13380765914917, 'loss_CE': 0.9990037083625793, 'loss_cal': 1.3480396270751953, 'acc_seen': 0.5809220671653748, 'acc_novel': 0.5495595335960388, 'H': 0.5648057607873046, 'acc_zs': 0.648659348487854}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1269, 'loss': 0.8725115060806274, 'loss_CE': 0.7155832648277283, 'loss_cal': 1.5692821741104126, 'acc_seen': 0.5809220671653748, 'acc_novel': 0.5495595335960388, 'H': 0.5648057607873046, 'acc_zs': 0.648659348487854}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1410, 'loss': 0.7587224841117859, 'loss_CE': 0.6123248338699341, 'loss_cal': 1.463976263999939, 'acc_seen': 0.5878117680549622, 'acc_novel': 0.5457720160484314, 'H': 0.5660123551645453, 'acc_zs': 0.6505881547927856}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1551, 'loss': 0.9522080421447754, 'loss_CE': 0.8048012852668762, 'loss_cal': 1.474067211151123, 'acc_seen': 0.5906508564949036, 'acc_novel': 0.5587936639785767, 'H': 0.5742807945126683, 'acc_zs': 0.6692891120910645}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1692, 'loss': 1.0499943494796753, 'loss_CE': 0.9086294174194336, 'loss_cal': 1.413649559020996, 'acc_seen': 0.5906508564949036, 'acc_novel': 0.5587936639785767, 'H': 0.5742807945126683, 'acc_zs': 0.6692891120910645}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1833, 'loss': 0.762641429901123, 'loss_CE': 0.6273956298828125, 'loss_cal': 1.352458119392395, 'acc_seen': 0.591661274433136, 'acc_novel': 0.5631834864616394, 'H': 0.5770712577531473, 'acc_zs': 0.6591776013374329}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1974, 'loss': 0.8310383558273315, 'loss_CE': 0.6961447596549988, 'loss_cal': 1.3489360809326172, 'acc_seen': 0.591661274433136, 'acc_novel': 0.5631834864616394, 'H': 0.5770712577531473, 'acc_zs': 0.6591776013374329}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2115, 'loss': 0.6843529939651489, 'loss_CE': 0.5164155960083008, 'loss_cal': 1.6793742179870605, 'acc_seen': 0.5928998589515686, 'acc_novel': 0.564140260219574, 'H': 0.5781626326884738, 'acc_zs': 0.6699299812316895}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2256, 'loss': 0.7171906232833862, 'loss_CE': 0.5732181668281555, 'loss_cal': 1.4397245645523071, 'acc_seen': 0.5928998589515686, 'acc_novel': 0.564140260219574, 'H': 0.5781626326884738, 'acc_zs': 0.6699299812316895}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2397, 'loss': 0.7362838387489319, 'loss_CE': 0.6180073618888855, 'loss_cal': 1.1827645301818848, 'acc_seen': 0.5928998589515686, 'acc_novel': 0.564140260219574, 'H': 0.5781626326884738, 'acc_zs': 0.6699299812316895}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2538, 'loss': 0.7740883827209473, 'loss_CE': 0.6076768040657043, 'loss_cal': 1.66411554813385, 'acc_seen': 0.5928998589515686, 'acc_novel': 0.564140260219574, 'H': 0.5781626326884738, 'acc_zs': 0.6699299812316895}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2679, 'loss': 1.0031527280807495, 'loss_CE': 0.8608006834983826, 'loss_cal': 1.423520803451538, 'acc_seen': 0.5928998589515686, 'acc_novel': 0.564140260219574, 'H': 0.5781626326884738, 'acc_zs': 0.6699299812316895}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2820, 'loss': 0.8257772922515869, 'loss_CE': 0.7006416320800781, 'loss_cal': 1.251356840133667, 'acc_seen': 0.5979835391044617, 'acc_novel': 0.5665391087532043, 'H': 0.5818367928121845, 'acc_zs': 0.6588420867919922}\n"
     ]
    }
   ],
   "source": [
    "best_performance = [0,0,0,0]\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
