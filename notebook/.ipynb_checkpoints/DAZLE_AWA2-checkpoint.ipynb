{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-1])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)\n",
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.AWA2DataLoader import AWA2DataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_GPU = 0\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "AWA2\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Balance dataloader\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/AWA2/feature_map_ResNet_101_AWA2.hdf5\n",
      "Expert Attr\n",
      "threshold at zero attribute with negative value\n",
      "Finish loading data in  175.6969\n"
     ]
    }
   ],
   "source": [
    "dataloader = AWA2DataLoader(NFS_path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = nn.Parameter(F.normalize(torch.tensor(att)),requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "normalize V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Add one smoothing\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "{'pmp': {'init_lambda': 0.1, 'final_lambda': 0.1, 'phase': 0.8}, 'desired_mass': {'init_lambda': -1, 'final_lambda': -1, 'phase': 0.8}}\n",
      "\t V\n",
      "\t W_1\n",
      "\t W_2\n",
      "\t W_3\n",
      "default lr ['V'] 1x lr ['W_1', 'W_2', 'W_3']\n",
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_ 0.1\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.0 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 214#214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att#dataloader.normalize_att#\n",
    "att[att<0] = 0\n",
    "normalize_att = dataloader.normalize_att\n",
    "#assert (att.min().item() == 0 and att.max().item() == 1)\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1#0.1\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1#unseenclass.size(0)/(seenclass.size(0)+unseenclass.size(0))\n",
    "report_interval = niters//nepoches#10000//batch_size#\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=True,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True)\n",
    "model.to(device)\n",
    "\n",
    "setup = {'pmp':{'init_lambda':0.1,'final_lambda':0.1,'phase':0.8},\n",
    "         'desired_mass':{'init_lambda':-1,'final_lambda':-1,'phase':0.8}}\n",
    "print(setup)\n",
    "\n",
    "params_to_update = []\n",
    "params_names = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        params_names.append(name)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.#0.#\n",
    "#%%\n",
    "lr_seperator = 1\n",
    "lr_factor = 1\n",
    "print('default lr {} {}x lr {}'.format(params_names[:lr_seperator],lr_factor,params_names[lr_seperator:]))\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss': 3.955843210220337, 'loss_CE': 3.9093644618988037, 'loss_cal': 0.46478742361068726, 'acc_seen': 0.0, 'acc_novel': 0.12085922807455063, 'H': 0.0, 'acc_zs': 0.12085922807455063}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 470, 'loss': 1.3514924049377441, 'loss_CE': 1.2345727682113647, 'loss_cal': 1.169196605682373, 'acc_seen': 0.5430148839950562, 'acc_novel': 0.6056535840034485, 'H': 0.5726263405347534, 'acc_zs': 0.646105170249939}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 940, 'loss': 0.9182254672050476, 'loss_CE': 0.7962093949317932, 'loss_cal': 1.2201608419418335, 'acc_seen': 0.7086815237998962, 'acc_novel': 0.6011627912521362, 'H': 0.6505093132987168, 'acc_zs': 0.6683744192123413}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1410, 'loss': 0.7537283301353455, 'loss_CE': 0.6223986744880676, 'loss_cal': 1.3132964372634888, 'acc_seen': 0.7395804524421692, 'acc_novel': 0.5977693200111389, 'H': 0.6611561361974528, 'acc_zs': 0.6678923964500427}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1880, 'loss': 0.662609338760376, 'loss_CE': 0.5255433917045593, 'loss_cal': 1.370659351348877, 'acc_seen': 0.7518362998962402, 'acc_novel': 0.6027500033378601, 'H': 0.6690889036601549, 'acc_zs': 0.6755213737487793}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2350, 'loss': 0.6536160707473755, 'loss_CE': 0.5199357271194458, 'loss_cal': 1.3368035554885864, 'acc_seen': 0.7530007362365723, 'acc_novel': 0.6061833500862122, 'H': 0.6716625268092912, 'acc_zs': 0.6787406206130981}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2820, 'loss': 0.6108418107032776, 'loss_CE': 0.48195621371269226, 'loss_cal': 1.2888559103012085, 'acc_seen': 0.7584196925163269, 'acc_novel': 0.5963899493217468, 'H': 0.6677157705650391, 'acc_zs': 0.6720418930053711}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3290, 'loss': 0.5895015001296997, 'loss_CE': 0.45069620013237, 'loss_cal': 1.388053059577942, 'acc_seen': 0.7628469467163086, 'acc_novel': 0.6038219332695007, 'H': 0.6740823983052283, 'acc_zs': 0.6789496541023254}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3760, 'loss': 0.644405722618103, 'loss_CE': 0.511443555355072, 'loss_cal': 1.3296215534210205, 'acc_seen': 0.7615612745285034, 'acc_novel': 0.5971595048904419, 'H': 0.6694142910446371, 'acc_zs': 0.6751786470413208}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 4230, 'loss': 0.5973643064498901, 'loss_CE': 0.462933212518692, 'loss_cal': 1.3443106412887573, 'acc_seen': 0.7598896622657776, 'acc_novel': 0.599858283996582, 'H': 0.6704567711780036, 'acc_zs': 0.6758412718772888}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 4700, 'loss': 0.6409440636634827, 'loss_CE': 0.5101036429405212, 'loss_cal': 1.3084039688110352, 'acc_seen': 0.7627507448196411, 'acc_novel': 0.592376708984375, 'H': 0.6668535490492484, 'acc_zs': 0.6711747050285339}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 5170, 'loss': 0.6012772917747498, 'loss_CE': 0.4705732464790344, 'loss_cal': 1.3070402145385742, 'acc_seen': 0.7630799412727356, 'acc_novel': 0.5875608921051025, 'H': 0.6639158538105465, 'acc_zs': 0.6654706597328186}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 5640, 'loss': 0.6804268956184387, 'loss_CE': 0.5569705963134766, 'loss_cal': 1.2345629930496216, 'acc_seen': 0.7618116736412048, 'acc_novel': 0.5960699319839478, 'H': 0.6688256628718839, 'acc_zs': 0.6696000099182129}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 6110, 'loss': 0.5830560922622681, 'loss_CE': 0.45000630617141724, 'loss_cal': 1.3304975032806396, 'acc_seen': 0.7579924464225769, 'acc_novel': 0.5965026021003723, 'H': 0.6676207006537956, 'acc_zs': 0.6715385913848877}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 6580, 'loss': 0.680651843547821, 'loss_CE': 0.566943883895874, 'loss_cal': 1.1370794773101807, 'acc_seen': 0.7612912058830261, 'acc_novel': 0.5903739929199219, 'H': 0.6650264124429927, 'acc_zs': 0.6697301268577576}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 7050, 'loss': 0.5572313666343689, 'loss_CE': 0.42874494194984436, 'loss_cal': 1.2848644256591797, 'acc_seen': 0.7553392648696899, 'acc_novel': 0.5897595286369324, 'H': 0.6623580824858158, 'acc_zs': 0.6678545475006104}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 7520, 'loss': 0.5773841142654419, 'loss_CE': 0.4520866870880127, 'loss_cal': 1.2529743909835815, 'acc_seen': 0.760007381439209, 'acc_novel': 0.5851300954818726, 'H': 0.6612011029331284, 'acc_zs': 0.6654127836227417}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 7990, 'loss': 0.6745968461036682, 'loss_CE': 0.5561791658401489, 'loss_cal': 1.1841765642166138, 'acc_seen': 0.7649677991867065, 'acc_novel': 0.5844373106956482, 'H': 0.6626263974417954, 'acc_zs': 0.6645075678825378}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 8460, 'loss': 0.549967885017395, 'loss_CE': 0.4177410304546356, 'loss_cal': 1.3222687244415283, 'acc_seen': 0.7644033432006836, 'acc_novel': 0.5829253196716309, 'H': 0.6614422679072035, 'acc_zs': 0.6673573851585388}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 8930, 'loss': 0.6991280317306519, 'loss_CE': 0.5688791275024414, 'loss_cal': 1.302489161491394, 'acc_seen': 0.7597586512565613, 'acc_novel': 0.5775426030158997, 'H': 0.6562365625676204, 'acc_zs': 0.6591876149177551}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 9400, 'loss': 0.6614841818809509, 'loss_CE': 0.5430386662483215, 'loss_cal': 1.184455394744873, 'acc_seen': 0.755203366279602, 'acc_novel': 0.5807591676712036, 'H': 0.6565921832044178, 'acc_zs': 0.6606995463371277}\n"
     ]
    }
   ],
   "source": [
    "best_performance = None\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H.item() > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
