{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/notebook\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-1])\n",
    "sys.path.insert(0,parent)\n",
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "from core.DAZLE import DAZLE\n",
    "from core.DeepFashionDataLoader import DeepFashionDataLoader\n",
    "from core.helper_func import eval_zs_gzsl,get_lr,get_attr_entropy#get_attribute_attention_stats\n",
    "from global_setting import NFS_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_GPU = 7\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "DeepFashion\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/DeepFashion/feature_map_ResNet_101_DeepFashion_sep_seen_samples.hdf5\n",
      "Finish loading data in  551.489919\n",
      "Balance dataloader\n",
      "Partition size 10000\n",
      "Excluding non-sample classes\n",
      "------------------------------\n",
      "DeepFashion\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataloader = DeepFashionDataLoader(NFS_path,device,is_balance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomize seed 214\n",
      "seeker  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = nn.Parameter(F.normalize(torch.tensor(att)),requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Add one smoothing\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "\t V\n",
      "\t W_1\n",
      "\t W_2\n",
      "\t W_3\n",
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_ 0.1\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.9 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "dataloader.idx_b = 0\n",
    "print('Randomize seed {}'.format(seed))\n",
    "#%%\n",
    "batch_size = 50\n",
    "nepoches = 1\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att#dataloader.normalize_att#\n",
    "normalize_att = dataloader.att\n",
    "#assert (att.min().item() == 0 and att.max().item() == 1)\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1\n",
    "bias = 0.\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "dataloader.seeker[:] = 0\n",
    "print('seeker ',dataloader.seeker)\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1#unseenclass.size(0)/(seenclass.size(0)+unseenclass.size(0))\n",
    "report_interval = 200#niters//nepoches\n",
    "#%%\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=False,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True,non_linear_act=False)\n",
    "model.to(device)\n",
    "#%%\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.9#0.#\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "#%%\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 33.30335600000001\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 0, 'loss': 4.189703941345215, 'loss_CE': 4.158728122711182, 'loss_cal': 0.3097580075263977, 'acc_seen': 0, 'acc_novel': 0, 'H': 0, 'acc_zs': 0}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 200, 'loss': 1.8229509592056274, 'loss_CE': 1.7100118398666382, 'loss_cal': 1.1293911933898926, 'acc_seen': 0.3145444989204407, 'acc_novel': 0.19587568938732147, 'H': 0.24141529657475722, 'acc_zs': 0.3387182056903839}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 35.98181899999997\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 400, 'loss': 1.8814144134521484, 'loss_CE': 1.7609858512878418, 'loss_cal': 1.2042860984802246, 'acc_seen': 0.37800484895706177, 'acc_novel': 0.1890132874250412, 'H': 0.25201288840550584, 'acc_zs': 0.36421096324920654}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 600, 'loss': 1.2690142393112183, 'loss_CE': 1.1435215473175049, 'loss_cal': 1.2549272775650024, 'acc_seen': 0.36895838379859924, 'acc_novel': 0.20752708613872528, 'H': 0.2656402018406512, 'acc_zs': 0.36523857712745667}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 36.09313999999995\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 800, 'loss': 1.343553066253662, 'loss_CE': 1.224740982055664, 'loss_cal': 1.1881206035614014, 'acc_seen': 0.36895838379859924, 'acc_novel': 0.20752708613872528, 'H': 0.2656402018406512, 'acc_zs': 0.36523857712745667}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 1000, 'loss': 1.1187831163406372, 'loss_CE': 0.9923740029335022, 'loss_cal': 1.2640912532806396, 'acc_seen': 0.36895838379859924, 'acc_novel': 0.20752708613872528, 'H': 0.2656402018406512, 'acc_zs': 0.36523857712745667}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 35.591203000000064\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 1200, 'loss': 1.322019100189209, 'loss_CE': 1.2000318765640259, 'loss_cal': 1.2198721170425415, 'acc_seen': 0.36895838379859924, 'acc_novel': 0.20752708613872528, 'H': 0.2656402018406512, 'acc_zs': 0.36523857712745667}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 1400, 'loss': 1.013647198677063, 'loss_CE': 0.8876954913139343, 'loss_cal': 1.2595171928405762, 'acc_seen': 0.38100937008857727, 'acc_novel': 0.21498239040374756, 'H': 0.27487059579550444, 'acc_zs': 0.3899243175983429}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 37.208722999999964\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 1600, 'loss': 1.5902624130249023, 'loss_CE': 1.4395182132720947, 'loss_cal': 1.5074422359466553, 'acc_seen': 0.38100937008857727, 'acc_novel': 0.21498239040374756, 'H': 0.27487059579550444, 'acc_zs': 0.3899243175983429}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 1800, 'loss': 1.0743675231933594, 'loss_CE': 0.9522173404693604, 'loss_cal': 1.2215015888214111, 'acc_seen': 0.38100937008857727, 'acc_novel': 0.21498239040374756, 'H': 0.27487059579550444, 'acc_zs': 0.3899243175983429}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 31.795719000000076\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n",
      "{'iter': 2000, 'loss': 1.172136902809143, 'loss_CE': 1.0547785758972168, 'loss_cal': 1.1735827922821045, 'acc_seen': 0.38100937008857727, 'acc_novel': 0.21498239040374756, 'H': 0.27487059579550444, 'acc_zs': 0.3899243175983429}\n",
      "------------------------------\n",
      "bias_seen -0.0 bias_unseen 0.0\n"
     ]
    }
   ],
   "source": [
    "best_performance = [0,0,0,0]\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
