{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-1])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)\n",
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.CUBDataLoader import CUBDataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention,eval_zs_gzsl#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path\n",
    "import importlib\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_GPU = 4\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "CUB\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/CUB/feature_map_ResNet_101_CUB.hdf5\n",
      "Expert Attr\n",
      "Finish loading data in  65.865992\n"
     ]
    }
   ],
   "source": [
    "dataloader = CUBDataLoader(NFS_path,device,is_unsupervised_attr=False,is_balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.augment_img_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    lr = []\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr.append(param_group['lr'])\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_DenseAttentionZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = nn.Parameter(F.normalize(torch.tensor(att)),requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Add one smoothing\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "{'pmp': {'init_lambda': 0.1, 'final_lambda': 0.1, 'phase': 0.8}, 'desired_mass': {'init_lambda': -1, 'final_lambda': -1, 'phase': 0.8}}\n",
      "\t V\n",
      "\t W_1\n",
      "\t W_2\n",
      "\t W_3\n",
      "default lr ['V'] 1x lr ['W_1', 'W_2', 'W_3']\n",
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_ 0.1\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.9 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "seed = 214#215#\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att\n",
    "normalize_att = dataloader.normalize_att\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_ = 0.1#0.1\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = False\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1\n",
    "report_interval = niters//nepoches\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_,\n",
    "            trainable_w2v,normalize_V=False,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=True)\n",
    "model.to(device)\n",
    "\n",
    "setup = {'pmp':{'init_lambda':0.1,'final_lambda':0.1,'phase':0.8},\n",
    "         'desired_mass':{'init_lambda':-1,'final_lambda':-1,'phase':0.8}}\n",
    "print(setup)\n",
    "#scheduler = Scheduler(model,niters,batch_size,report_interval,setup)\n",
    "\n",
    "params_to_update = []\n",
    "params_names = []\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        params_names.append(name)\n",
    "        print(\"\\t\",name)\n",
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.9#0.#\n",
    "#%%\n",
    "lr_seperator = 1\n",
    "lr_factor = 1\n",
    "print('default lr {} {}x lr {}'.format(params_names[:lr_seperator],lr_factor,params_names[lr_seperator:]))\n",
    "optimizer  = optim.RMSprop( params_to_update ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_ {}'.format(lambda_))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss': 5.359918594360352, 'loss_CE': 5.324845314025879, 'loss_cal': 0.35073134303092957, 'acc_seen': 0.0, 'acc_novel': 0.028149642050266266, 'H': 0.0, 'acc_zs': 0.028149642050266266}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 141, 'loss': 1.7408692836761475, 'loss_CE': 1.6522305011749268, 'loss_cal': 0.886387825012207, 'acc_seen': 0.32359516620635986, 'acc_novel': 0.4776896834373474, 'H': 0.38582552153757876, 'acc_zs': 0.5216538310050964}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 282, 'loss': 1.8812105655670166, 'loss_CE': 1.781968355178833, 'loss_cal': 0.9924225807189941, 'acc_seen': 0.4477420151233673, 'acc_novel': 0.5177309513092041, 'H': 0.4801996689507735, 'acc_zs': 0.5883574485778809}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 423, 'loss': 1.3246947526931763, 'loss_CE': 1.2124630212783813, 'loss_cal': 1.1223170757293701, 'acc_seen': 0.4954133629798889, 'acc_novel': 0.5341808199882507, 'H': 0.5140672331827328, 'acc_zs': 0.6073644161224365}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 564, 'loss': 1.2282207012176514, 'loss_CE': 1.0951151847839355, 'loss_cal': 1.3310551643371582, 'acc_seen': 0.5294926166534424, 'acc_novel': 0.5410490036010742, 'H': 0.5352084351215805, 'acc_zs': 0.6311136484146118}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 705, 'loss': 0.9004603624343872, 'loss_CE': 0.7800907492637634, 'loss_cal': 1.203696370124817, 'acc_seen': 0.5507971048355103, 'acc_novel': 0.5552099347114563, 'H': 0.5529947164536048, 'acc_zs': 0.6434260606765747}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 846, 'loss': 0.9555374979972839, 'loss_CE': 0.8249633312225342, 'loss_cal': 1.305741786956787, 'acc_seen': 0.5508449673652649, 'acc_novel': 0.5576741695404053, 'H': 0.5542385322790803, 'acc_zs': 0.6433893442153931}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 987, 'loss': 1.0262272357940674, 'loss_CE': 0.8983624577522278, 'loss_cal': 1.2786474227905273, 'acc_seen': 0.5724098086357117, 'acc_novel': 0.550646185874939, 'H': 0.5613171194015102, 'acc_zs': 0.6414066553115845}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1128, 'loss': 0.9880400896072388, 'loss_CE': 0.8556456565856934, 'loss_cal': 1.3239439725875854, 'acc_seen': 0.58404940366745, 'acc_novel': 0.55502849817276, 'H': 0.5691692602455817, 'acc_zs': 0.6533961296081543}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1269, 'loss': 1.3570982217788696, 'loss_CE': 1.2437424659729004, 'loss_cal': 1.1335580348968506, 'acc_seen': 0.5843819975852966, 'acc_novel': 0.5457586646080017, 'H': 0.5644103416368504, 'acc_zs': 0.6561080813407898}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1410, 'loss': 0.96678227186203, 'loss_CE': 0.8223214745521545, 'loss_cal': 1.4446079730987549, 'acc_seen': 0.5904279351234436, 'acc_novel': 0.5443682670593262, 'H': 0.566463354826771, 'acc_zs': 0.652599573135376}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1551, 'loss': 0.9133927822113037, 'loss_CE': 0.7582013010978699, 'loss_cal': 1.5519144535064697, 'acc_seen': 0.5882644057273865, 'acc_novel': 0.5592772364616394, 'H': 0.5734047097695948, 'acc_zs': 0.6690823435783386}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1692, 'loss': 0.8834778070449829, 'loss_CE': 0.7500545978546143, 'loss_cal': 1.3342318534851074, 'acc_seen': 0.5882753729820251, 'acc_novel': 0.5636724829673767, 'H': 0.5757111981150347, 'acc_zs': 0.6611422896385193}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1833, 'loss': 0.7711523771286011, 'loss_CE': 0.6243326663970947, 'loss_cal': 1.4681968688964844, 'acc_seen': 0.5921789407730103, 'acc_novel': 0.5641711354255676, 'H': 0.5778358513873552, 'acc_zs': 0.6579108238220215}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1974, 'loss': 0.8989297151565552, 'loss_CE': 0.7760301828384399, 'loss_cal': 1.2289955615997314, 'acc_seen': 0.5972912907600403, 'acc_novel': 0.5547809600830078, 'H': 0.5752518307677172, 'acc_zs': 0.6579270362854004}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2115, 'loss': 0.7019197940826416, 'loss_CE': 0.554714024066925, 'loss_cal': 1.472057819366455, 'acc_seen': 0.5906908512115479, 'acc_novel': 0.5644384622573853, 'H': 0.577266340382451, 'acc_zs': 0.6696022152900696}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2256, 'loss': 0.7666165232658386, 'loss_CE': 0.6245309114456177, 'loss_cal': 1.420856237411499, 'acc_seen': 0.5956068634986877, 'acc_novel': 0.551080584526062, 'H': 0.5724792384358722, 'acc_zs': 0.6525691747665405}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2397, 'loss': 0.6812198162078857, 'loss_CE': 0.5535007119178772, 'loss_cal': 1.2771908044815063, 'acc_seen': 0.5830579996109009, 'acc_novel': 0.5644914507865906, 'H': 0.5736245283008578, 'acc_zs': 0.6639742851257324}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2538, 'loss': 1.0034352540969849, 'loss_CE': 0.8618229627609253, 'loss_cal': 1.4161230325698853, 'acc_seen': 0.6021144986152649, 'acc_novel': 0.5494396686553955, 'H': 0.5745723475533631, 'acc_zs': 0.660067617893219}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2679, 'loss': 0.6630265712738037, 'loss_CE': 0.5183085203170776, 'loss_cal': 1.4471802711486816, 'acc_seen': 0.604638397693634, 'acc_novel': 0.5562275648117065, 'H': 0.5794235586250726, 'acc_zs': 0.6534966230392456}\n",
      "------------------------------\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2820, 'loss': 0.7309019565582275, 'loss_CE': 0.5841178894042969, 'loss_cal': 1.4678409099578857, 'acc_seen': 0.6003713011741638, 'acc_novel': 0.5679289698600769, 'H': 0.5836996927297703, 'acc_zs': 0.662609338760376}\n"
     ]
    }
   ],
   "source": [
    "best_performance = [0,0,0,0]\n",
    "for i in range(0,niters):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "    out_package = model(batch_feature)\n",
    "    \n",
    "    in_package = out_package\n",
    "    in_package['batch_label'] = batch_label\n",
    "    \n",
    "    out_package=model.compute_loss(in_package)\n",
    "    loss,loss_CE,loss_cal = out_package['loss'],out_package['loss_CE'],out_package['loss_cal']\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%report_interval==0:\n",
    "        print('-'*30)\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss':loss.item(), 'loss_CE':loss_CE.item(),\n",
    "                         'loss_cal': loss_cal.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
